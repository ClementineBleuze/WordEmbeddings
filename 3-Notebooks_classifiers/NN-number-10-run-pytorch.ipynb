{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data: NOUN, ADJ, and both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nouns_we = pd.read_csv('../Data/FlauBERT_WE/all_nouns_we.csv').drop(columns=[\"gender\"])\n",
    "df_adj_we = pd.read_csv('../Data/FlauBERT_WE/all_adjectives_we.csv').drop(columns = \"gender\")\n",
    "df_both_we = pd.concat([df_nouns_we, df_adj_we], ignore_index=True)\n",
    "# target : number\n",
    "Y_nb_N = df_nouns_we[\"number\"].apply(lambda x: 1 if x == \"singular\" else 0)\n",
    "Y_nb_A = df_adj_we[\"number\"].apply(lambda x: 1 if x == \"singular\" else 0)\n",
    "Y_nb_both = df_both_we[\"number\"].apply(lambda x: 1 if x == \"singular\" else 0)\n",
    "# features : word embeddings dimensions\n",
    "X_nb_N = df_nouns_we.drop(columns=[\"Word\", \"number\"])\n",
    "X_nb_A = df_adj_we.drop(columns=[\"Word\", \"number\"])\n",
    "X_nb_both = df_both_we.drop(columns = [\"Word\", \"number\"] )\n",
    "\n",
    "# normalize data to be between 0 and 1\n",
    "X_nb_N = (X_nb_N - X_nb_N.min()) / (X_nb_N.max() - X_nb_N.min())\n",
    "X_nb_A = (X_nb_A - X_nb_A.min()) / (X_nb_A.max() - X_nb_A.min())\n",
    "X_nb_both = (X_nb_both - X_nb_both.min()) / (X_nb_both.max() - X_nb_both.min())\n",
    "\n",
    "\n",
    "# split data into train and test sets\n",
    "X_nb_N_train, X_nb_N_test, Y_nb_N_train, Y_nb_N_test = train_test_split(X_nb_N, Y_nb_N, test_size=0.2, random_state=42)\n",
    "X_nb_A_train, X_nb_A_test, Y_nb_A_train, Y_nb_A_test = train_test_split(X_nb_A, Y_nb_A, test_size=0.2, random_state=42)\n",
    "X_nb_both_train, X_nb_both_test, Y_nb_both_train, Y_nb_both_test = train_test_split(X_nb_both, Y_nb_both, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = [X_nb_N_train, X_nb_A_train, X_nb_both_train]\n",
    "test_feature = [X_nb_N_test, X_nb_A_test, X_nb_both_test]\n",
    "train_target = [Y_nb_N_train, Y_nb_A_train, Y_nb_both_train]\n",
    "test_target = [Y_nb_N_test, Y_nb_A_test, Y_nb_both_test]\n",
    "\n",
    "names = ['Number: Noun', 'Number: Adjs', 'Number: Both']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensors\n",
    "train_feature = [torch.tensor(x.values).float() for x in train_feature]\n",
    "test_feature = [torch.tensor(x.values).float() for x in test_feature]\n",
    "train_target = [torch.tensor(x.values).long() for x in train_target]\n",
    "test_target = [torch.tensor(x.values).long() for x in test_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# the TensorDataset is a ready to use class to represent your data as list of tensors. \n",
    "# Note that input_features and labels must match on the length of the first dimension\n",
    "train_sets = [TensorDataset(X_train, Y_train) for X_train, Y_train in zip(train_feature, train_target)]\n",
    "test_sets = [TensorDataset(X_valid, Y_valid) for X_valid, Y_valid in zip(test_feature, test_target)]\n",
    "\n",
    "# DataLoader shuffles and batches the data and load its in parallel using multiprocessing workers\n",
    "train_loaders = [DataLoader(train_set, batch_size=32, shuffle=True) for train_set in train_sets]\n",
    "test_loaders = [DataLoader(test_set, batch_size=32) for test_set in test_sets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [[], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training model:  Number: Noun, run 0\n",
      "--Epoch  0  Loss :  0.5339772701263428\n",
      "--Epoch  10  Loss :  0.3435690701007843\n",
      " Training model:  Number: Adjs, run 0\n",
      "--Epoch  0  Loss :  0.6323344111442566\n",
      "--Epoch  10  Loss :  0.35955438017845154\n",
      " Training model:  Number: Both, run 0\n",
      "--Epoch  0  Loss :  0.5656259655952454\n",
      "--Epoch  10  Loss :  0.3276616632938385\n",
      " Training model:  Number: Noun, run 1\n",
      "--Epoch  0  Loss :  0.5480201840400696\n",
      "--Epoch  10  Loss :  0.35430285334587097\n",
      " Training model:  Number: Adjs, run 1\n",
      "--Epoch  0  Loss :  0.6420716643333435\n",
      "--Epoch  10  Loss :  0.37520483136177063\n",
      " Training model:  Number: Both, run 1\n",
      "--Epoch  0  Loss :  0.6836819052696228\n",
      "--Epoch  10  Loss :  0.3508302867412567\n",
      " Training model:  Number: Noun, run 2\n",
      "--Epoch  0  Loss :  0.5583521127700806\n",
      "--Epoch  10  Loss :  0.38170331716537476\n",
      " Training model:  Number: Adjs, run 2\n",
      "--Epoch  0  Loss :  0.5966718792915344\n",
      "--Epoch  10  Loss :  0.3695962131023407\n",
      " Training model:  Number: Both, run 2\n",
      "--Epoch  0  Loss :  0.5465124249458313\n",
      "--Epoch  10  Loss :  0.32893112301826477\n",
      " Training model:  Number: Noun, run 3\n",
      "--Epoch  0  Loss :  0.5751935839653015\n",
      "--Epoch  10  Loss :  0.3934526741504669\n",
      " Training model:  Number: Adjs, run 3\n",
      "--Epoch  0  Loss :  0.5958230495452881\n",
      "--Epoch  10  Loss :  0.42195263504981995\n",
      " Training model:  Number: Both, run 3\n",
      "--Epoch  0  Loss :  0.6093443632125854\n",
      "--Epoch  10  Loss :  0.3270179331302643\n",
      " Training model:  Number: Noun, run 4\n",
      "--Epoch  0  Loss :  0.5781543254852295\n",
      "--Epoch  10  Loss :  0.35466936230659485\n",
      " Training model:  Number: Adjs, run 4\n",
      "--Epoch  0  Loss :  0.6029714941978455\n",
      "--Epoch  10  Loss :  0.42655104398727417\n",
      " Training model:  Number: Both, run 4\n",
      "--Epoch  0  Loss :  0.5319894552230835\n",
      "--Epoch  10  Loss :  0.3247339427471161\n",
      " Training model:  Number: Noun, run 5\n",
      "--Epoch  0  Loss :  0.5388489365577698\n",
      "--Epoch  10  Loss :  0.3955293893814087\n",
      " Training model:  Number: Adjs, run 5\n",
      "--Epoch  0  Loss :  0.6060453057289124\n",
      "--Epoch  10  Loss :  0.40474042296409607\n",
      " Training model:  Number: Both, run 5\n",
      "--Epoch  0  Loss :  0.5520179271697998\n",
      "--Epoch  10  Loss :  0.33467531204223633\n",
      " Training model:  Number: Noun, run 6\n",
      "--Epoch  0  Loss :  0.601459801197052\n",
      "--Epoch  10  Loss :  0.35046911239624023\n",
      " Training model:  Number: Adjs, run 6\n",
      "--Epoch  0  Loss :  0.5414342284202576\n",
      "--Epoch  10  Loss :  0.392938494682312\n",
      " Training model:  Number: Both, run 6\n",
      "--Epoch  0  Loss :  0.5946194529533386\n",
      "--Epoch  10  Loss :  0.37088891863822937\n",
      " Training model:  Number: Noun, run 7\n",
      "--Epoch  0  Loss :  0.5327560305595398\n",
      "--Epoch  10  Loss :  0.35578298568725586\n",
      " Training model:  Number: Adjs, run 7\n",
      "--Epoch  0  Loss :  0.5761398673057556\n",
      "--Epoch  10  Loss :  0.37461337447166443\n",
      " Training model:  Number: Both, run 7\n",
      "--Epoch  0  Loss :  0.5721492171287537\n",
      "--Epoch  10  Loss :  0.344701886177063\n",
      " Training model:  Number: Noun, run 8\n",
      "--Epoch  0  Loss :  0.5416790843009949\n",
      "--Epoch  10  Loss :  0.3489137291908264\n",
      " Training model:  Number: Adjs, run 8\n",
      "--Epoch  0  Loss :  0.6548070311546326\n",
      "--Epoch  10  Loss :  0.379184365272522\n",
      " Training model:  Number: Both, run 8\n",
      "--Epoch  0  Loss :  0.5546350479125977\n",
      "--Epoch  10  Loss :  0.34257230162620544\n",
      " Training model:  Number: Noun, run 9\n",
      "--Epoch  0  Loss :  0.5726543068885803\n",
      "--Epoch  10  Loss :  0.3825297951698303\n",
      " Training model:  Number: Adjs, run 9\n",
      "--Epoch  0  Loss :  0.5473660826683044\n",
      "--Epoch  10  Loss :  0.39497435092926025\n",
      " Training model:  Number: Both, run 9\n",
      "--Epoch  0  Loss :  0.5035380125045776\n",
      "--Epoch  10  Loss :  0.33020710945129395\n"
     ]
    }
   ],
   "source": [
    "# train the models \n",
    "\n",
    "for y in range(10):\n",
    "    for i in range(3):\n",
    "        print(f\" Training model:  {names[i]}, run {y}\")\n",
    "        \n",
    "        model = nn.Sequential(nn.Linear(512, 2), nn.Softmax(dim=1))\n",
    "        # define the loss function\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        # define the optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        # put the model in training mode\n",
    "        model.train()\n",
    "        for epoch in range(nb_epochs):\n",
    "            for X_train, Y_train in train_loaders[i]:\n",
    "                # compute the model output\n",
    "                Y_pred = model(X_train)\n",
    "                # calculate loss\n",
    "                loss = loss_fn(Y_pred, Y_train)\n",
    "                # reset the gradients\n",
    "                optimizer.zero_grad()\n",
    "                # backpropagation\n",
    "                loss.backward()\n",
    "                # update model weights\n",
    "                optimizer.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"--Epoch \", epoch, \" Loss : \", loss.item())\n",
    "        \n",
    "        weights[i].append(model[0].weight.data.numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.abs(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number: Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_weights = pd.DataFrame(columns=list(range(512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_weights['run'] = list(range(10))\n",
    "for r in range(10):\n",
    "    dims_sorted = [x[0] for x in sorted(enumerate(weights[0][r]), key=lambda x: abs(x[1]), reverse=True)]\n",
    "    for i in range(len(dims_sorted)):\n",
    "        noun_weights.iloc[r, dims_sorted[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>400</td>\n",
       "      <td>51</td>\n",
       "      <td>368</td>\n",
       "      <td>16</td>\n",
       "      <td>193</td>\n",
       "      <td>87</td>\n",
       "      <td>58</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>362</td>\n",
       "      <td>311</td>\n",
       "      <td>478</td>\n",
       "      <td>169</td>\n",
       "      <td>72</td>\n",
       "      <td>315</td>\n",
       "      <td>390</td>\n",
       "      <td>355</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245</td>\n",
       "      <td>134</td>\n",
       "      <td>382</td>\n",
       "      <td>48</td>\n",
       "      <td>363</td>\n",
       "      <td>21</td>\n",
       "      <td>207</td>\n",
       "      <td>82</td>\n",
       "      <td>68</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>362</td>\n",
       "      <td>313</td>\n",
       "      <td>434</td>\n",
       "      <td>131</td>\n",
       "      <td>93</td>\n",
       "      <td>337</td>\n",
       "      <td>351</td>\n",
       "      <td>394</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220</td>\n",
       "      <td>83</td>\n",
       "      <td>372</td>\n",
       "      <td>58</td>\n",
       "      <td>351</td>\n",
       "      <td>22</td>\n",
       "      <td>216</td>\n",
       "      <td>96</td>\n",
       "      <td>53</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>359</td>\n",
       "      <td>328</td>\n",
       "      <td>409</td>\n",
       "      <td>137</td>\n",
       "      <td>72</td>\n",
       "      <td>340</td>\n",
       "      <td>317</td>\n",
       "      <td>365</td>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281</td>\n",
       "      <td>103</td>\n",
       "      <td>410</td>\n",
       "      <td>48</td>\n",
       "      <td>425</td>\n",
       "      <td>16</td>\n",
       "      <td>227</td>\n",
       "      <td>95</td>\n",
       "      <td>43</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>398</td>\n",
       "      <td>352</td>\n",
       "      <td>407</td>\n",
       "      <td>135</td>\n",
       "      <td>68</td>\n",
       "      <td>348</td>\n",
       "      <td>371</td>\n",
       "      <td>296</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>236</td>\n",
       "      <td>97</td>\n",
       "      <td>428</td>\n",
       "      <td>61</td>\n",
       "      <td>354</td>\n",
       "      <td>17</td>\n",
       "      <td>215</td>\n",
       "      <td>88</td>\n",
       "      <td>48</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>413</td>\n",
       "      <td>365</td>\n",
       "      <td>380</td>\n",
       "      <td>159</td>\n",
       "      <td>72</td>\n",
       "      <td>307</td>\n",
       "      <td>346</td>\n",
       "      <td>303</td>\n",
       "      <td>91</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>238</td>\n",
       "      <td>97</td>\n",
       "      <td>374</td>\n",
       "      <td>59</td>\n",
       "      <td>356</td>\n",
       "      <td>22</td>\n",
       "      <td>202</td>\n",
       "      <td>102</td>\n",
       "      <td>51</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>417</td>\n",
       "      <td>303</td>\n",
       "      <td>423</td>\n",
       "      <td>148</td>\n",
       "      <td>82</td>\n",
       "      <td>313</td>\n",
       "      <td>337</td>\n",
       "      <td>359</td>\n",
       "      <td>76</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>241</td>\n",
       "      <td>122</td>\n",
       "      <td>429</td>\n",
       "      <td>66</td>\n",
       "      <td>370</td>\n",
       "      <td>23</td>\n",
       "      <td>207</td>\n",
       "      <td>104</td>\n",
       "      <td>73</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>401</td>\n",
       "      <td>358</td>\n",
       "      <td>432</td>\n",
       "      <td>135</td>\n",
       "      <td>69</td>\n",
       "      <td>379</td>\n",
       "      <td>317</td>\n",
       "      <td>319</td>\n",
       "      <td>78</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>221</td>\n",
       "      <td>119</td>\n",
       "      <td>423</td>\n",
       "      <td>66</td>\n",
       "      <td>417</td>\n",
       "      <td>24</td>\n",
       "      <td>265</td>\n",
       "      <td>93</td>\n",
       "      <td>60</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>314</td>\n",
       "      <td>349</td>\n",
       "      <td>374</td>\n",
       "      <td>175</td>\n",
       "      <td>72</td>\n",
       "      <td>369</td>\n",
       "      <td>344</td>\n",
       "      <td>353</td>\n",
       "      <td>94</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>243</td>\n",
       "      <td>119</td>\n",
       "      <td>356</td>\n",
       "      <td>48</td>\n",
       "      <td>369</td>\n",
       "      <td>22</td>\n",
       "      <td>201</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>399</td>\n",
       "      <td>301</td>\n",
       "      <td>464</td>\n",
       "      <td>123</td>\n",
       "      <td>85</td>\n",
       "      <td>313</td>\n",
       "      <td>351</td>\n",
       "      <td>343</td>\n",
       "      <td>74</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>266</td>\n",
       "      <td>129</td>\n",
       "      <td>427</td>\n",
       "      <td>56</td>\n",
       "      <td>356</td>\n",
       "      <td>28</td>\n",
       "      <td>203</td>\n",
       "      <td>94</td>\n",
       "      <td>57</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>353</td>\n",
       "      <td>326</td>\n",
       "      <td>460</td>\n",
       "      <td>132</td>\n",
       "      <td>93</td>\n",
       "      <td>321</td>\n",
       "      <td>329</td>\n",
       "      <td>357</td>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2   3    4   5    6    7   8   9  ...  503  504  505  506 507  \\\n",
       "0  256  128  400  51  368  16  193   87  58  59  ...  362  311  478  169  72   \n",
       "1  245  134  382  48  363  21  207   82  68  67  ...  362  313  434  131  93   \n",
       "2  220   83  372  58  351  22  216   96  53  52  ...  359  328  409  137  72   \n",
       "3  281  103  410  48  425  16  227   95  43  74  ...  398  352  407  135  68   \n",
       "4  236   97  428  61  354  17  215   88  48  59  ...  413  365  380  159  72   \n",
       "5  238   97  374  59  356  22  202  102  51  56  ...  417  303  423  148  82   \n",
       "6  241  122  429  66  370  23  207  104  73  71  ...  401  358  432  135  69   \n",
       "7  221  119  423  66  417  24  265   93  60  57  ...  314  349  374  175  72   \n",
       "8  243  119  356  48  369  22  201   89  66  53  ...  399  301  464  123  85   \n",
       "9  266  129  427  56  356  28  203   94  57  63  ...  353  326  460  132  93   \n",
       "\n",
       "   508  509  510 511 run  \n",
       "0  315  390  355  80   0  \n",
       "1  337  351  394  72   1  \n",
       "2  340  317  365  76   2  \n",
       "3  348  371  296  77   3  \n",
       "4  307  346  303  91   4  \n",
       "5  313  337  359  76   5  \n",
       "6  379  317  319  78   6  \n",
       "7  369  344  353  94   7  \n",
       "8  313  351  343  74   8  \n",
       "9  321  329  357  69   9  \n",
       "\n",
       "[10 rows x 513 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310     0.0\n",
       "54      1.0\n",
       "158     2.0\n",
       "285     3.0\n",
       "359     4.6\n",
       "172     5.3\n",
       "384     5.9\n",
       "495     6.3\n",
       "250     8.8\n",
       "182    10.1\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_weights.iloc[:, :512].mean().sort_values()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number: Adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_weights = pd.DataFrame(columns=list(range(512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_weights['run'] = list(range(10))\n",
    "for r in range(10):\n",
    "    dims_sorted = [x[0] for x in sorted(enumerate(weights[1][r]), key=lambda x: abs(x[1]), reverse=True)]\n",
    "    for i in range(len(dims_sorted)):\n",
    "        adj_weights.iloc[r, dims_sorted[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310    0.0\n",
       "285    1.3\n",
       "54     1.7\n",
       "384    3.9\n",
       "455    5.0\n",
       "495    6.6\n",
       "200    7.6\n",
       "360    8.2\n",
       "192    8.7\n",
       "25     9.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_weights.iloc[:, :512].mean().sort_values()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number: both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_weights = pd.DataFrame(columns=list(range(512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_weights['run'] = list(range(10))\n",
    "for r in range(10):\n",
    "    dims_sorted = [x[0] for x in sorted(enumerate(weights[2][r]), key=lambda x: abs(x[1]), reverse=True)]\n",
    "    for i in range(len(dims_sorted)):\n",
    "        both_weights.iloc[r, dims_sorted[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310    0.0\n",
       "54     1.0\n",
       "158    2.0\n",
       "285    3.0\n",
       "359    4.1\n",
       "384    5.1\n",
       "172    6.4\n",
       "495    6.7\n",
       "250    8.5\n",
       "200    9.4\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_weights.iloc[:, :512].mean().sort_values()[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3de7a084b318d7b8bf96005cb5db4da14a27f60df0465391ef48a4c336f03bfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
