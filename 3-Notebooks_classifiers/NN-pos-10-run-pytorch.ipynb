{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Classification (Perceptron) : Noun/ Not-Noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data: NOUN, ADJ, and both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = pd.read_csv('../Data/FlauBERT_WE/all_nouns_we.csv', index_col=0).drop(columns=['gender', 'number'])\n",
    "nouns['noun'] = 1\n",
    "nouns['verb'] = 0\n",
    "nouns['adj'] = 0\n",
    "\n",
    "verbs = pd.read_csv('../Data/FlauBERT_WE/all_verb_we.csv', index_col=0)\n",
    "verbs['noun'] = 0\n",
    "verbs['verb'] = 1\n",
    "verbs['adj'] = 0\n",
    "\n",
    "adjs = pd.read_csv('../Data/FlauBERT_WE/all_adjectives_we.csv', index_col=0).drop(columns=['gender', 'number'])\n",
    "adjs['noun'] = 0\n",
    "adjs['verb'] = 0\n",
    "adjs['adj'] = 1\n",
    "\n",
    "\n",
    "data = pd.concat([nouns, adjs, verbs])\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "normalized_data = (data - data.min())/(data.max() - data.min()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target : gender\n",
    "Y_N = np.asarray(normalized_data.noun)\n",
    "Y_V = np.asarray(normalized_data.verb)\n",
    "Y_A = np.asarray(normalized_data.adj)\n",
    "\n",
    "# features : word embeddings dimensions\n",
    "X = np.asarray(normalized_data.iloc[:, :512])\n",
    "\n",
    "# split data into train and test sets\n",
    "X_N_train, X_N_test, Y_N_train, Y_N_test = train_test_split(X, Y_N, test_size=0.2, random_state=42)\n",
    "X_A_train, X_A_test, Y_A_train, Y_A_test = train_test_split(X, Y_A, test_size=0.2, random_state=42)\n",
    "X_V_train, X_V_test, Y_V_train, Y_V_test = train_test_split(X, Y_V, test_size=0.2, random_state=42)\n",
    "\n",
    "names = ['Noun vs Not Noun', 'Adj vs Not Adj', 'Verb vs not Verb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = [X_N_train, X_A_train, X_V_train]\n",
    "test_features = [X_N_test, X_A_test, X_V_test]\n",
    "train_targets = [Y_N_train, Y_A_train, Y_V_train]\n",
    "test_targets = [Y_N_test, Y_A_test, Y_V_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensors\n",
    "train_features = [torch.tensor(x).float() for x in train_features]\n",
    "test_features = [torch.tensor(x).float() for x in test_features]\n",
    "train_targets = [torch.tensor(x).long() for x in train_targets]\n",
    "test_targets = [torch.tensor(x).long() for x in test_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# the TensorDataset is a ready to use class to represent your data as list of tensors. \n",
    "# Note that input_features and labels must match on the length of the first dimension\n",
    "train_sets = [TensorDataset(X_train, Y_train) for X_train, Y_train in zip(train_features, train_targets)]\n",
    "test_sets = [TensorDataset(X_valid, Y_valid) for X_valid, Y_valid in zip(test_features, test_targets)]\n",
    "\n",
    "# DataLoader shuffles and batches the data and load its in parallel using multiprocessing workers\n",
    "train_loaders = [DataLoader(train_set, batch_size=32, shuffle=True) for train_set in train_sets]\n",
    "test_loaders = [DataLoader(test_set, batch_size=32) for test_set in test_sets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNlist = []\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    # create a fully connected perceptron with 1 input layer (512 features) and 1 output layer (2 classes)\n",
    "    model = nn.Sequential(nn.Linear(512, 2), nn.Softmax(dim=1))\n",
    "    # define the loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # add the model to the list\n",
    "    NNlist.append([model, loss_fn, optimizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [[], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training model:  Noun vs Not Noun, run 0\n",
      "--Epoch  0  Loss :  0.5742295980453491\n",
      "--Epoch  10  Loss :  0.5159028768539429\n",
      " Training model:  Adj vs Not Adj, run 0\n",
      "--Epoch  0  Loss :  0.43832382559776306\n",
      "--Epoch  10  Loss :  0.438262015581131\n",
      " Training model:  Verb vs not Verb, run 0\n",
      "--Epoch  0  Loss :  0.6563544273376465\n",
      "--Epoch  10  Loss :  0.5942022800445557\n",
      " Training model:  Noun vs Not Noun, run 1\n",
      "--Epoch  0  Loss :  0.5531182885169983\n",
      "--Epoch  10  Loss :  0.5248103141784668\n",
      " Training model:  Adj vs Not Adj, run 1\n",
      "--Epoch  0  Loss :  0.7506886720657349\n",
      "--Epoch  10  Loss :  0.5007619261741638\n",
      " Training model:  Verb vs not Verb, run 1\n",
      "--Epoch  0  Loss :  0.536217212677002\n",
      "--Epoch  10  Loss :  0.5251181125640869\n",
      " Training model:  Noun vs Not Noun, run 2\n",
      "--Epoch  0  Loss :  0.5758458375930786\n",
      "--Epoch  10  Loss :  0.6964913606643677\n",
      " Training model:  Adj vs Not Adj, run 2\n",
      "--Epoch  0  Loss :  0.5007773041725159\n",
      "--Epoch  10  Loss :  0.4382621943950653\n",
      " Training model:  Verb vs not Verb, run 2\n",
      "--Epoch  0  Loss :  0.5605183243751526\n",
      "--Epoch  10  Loss :  0.5177885293960571\n",
      " Training model:  Noun vs Not Noun, run 3\n",
      "--Epoch  0  Loss :  0.6450486183166504\n",
      "--Epoch  10  Loss :  0.430223286151886\n",
      " Training model:  Adj vs Not Adj, run 3\n",
      "--Epoch  0  Loss :  0.6257513165473938\n",
      "--Epoch  10  Loss :  0.5007617473602295\n",
      " Training model:  Verb vs not Verb, run 3\n",
      "--Epoch  0  Loss :  0.5938848853111267\n",
      "--Epoch  10  Loss :  0.6384789347648621\n",
      " Training model:  Noun vs Not Noun, run 4\n",
      "--Epoch  0  Loss :  0.5447871685028076\n",
      "--Epoch  10  Loss :  0.4961395859718323\n",
      " Training model:  Adj vs Not Adj, run 4\n",
      "--Epoch  0  Loss :  0.5632768869400024\n",
      "--Epoch  10  Loss :  0.6882613897323608\n",
      " Training model:  Verb vs not Verb, run 4\n",
      "--Epoch  0  Loss :  0.5505780577659607\n",
      "--Epoch  10  Loss :  0.5212061405181885\n",
      " Training model:  Noun vs Not Noun, run 5\n",
      "--Epoch  0  Loss :  0.5810540914535522\n",
      "--Epoch  10  Loss :  0.6868099570274353\n",
      " Training model:  Adj vs Not Adj, run 5\n",
      "--Epoch  0  Loss :  0.5632562041282654\n",
      "--Epoch  10  Loss :  0.8132612705230713\n",
      " Training model:  Verb vs not Verb, run 5\n",
      "--Epoch  0  Loss :  0.5240078568458557\n",
      "--Epoch  10  Loss :  0.5262730717658997\n",
      " Training model:  Noun vs Not Noun, run 6\n",
      "--Epoch  0  Loss :  0.6655723452568054\n",
      "--Epoch  10  Loss :  0.6774322986602783\n",
      " Training model:  Adj vs Not Adj, run 6\n",
      "--Epoch  0  Loss :  0.5632672905921936\n",
      "--Epoch  10  Loss :  0.5632617473602295\n",
      " Training model:  Verb vs not Verb, run 6\n",
      "--Epoch  0  Loss :  0.6791543960571289\n",
      "--Epoch  10  Loss :  0.7031111717224121\n",
      " Training model:  Noun vs Not Noun, run 7\n",
      "--Epoch  0  Loss :  0.5442114472389221\n",
      "--Epoch  10  Loss :  0.4421580731868744\n",
      " Training model:  Adj vs Not Adj, run 7\n",
      "--Epoch  0  Loss :  0.5007998943328857\n",
      "--Epoch  10  Loss :  0.5007621049880981\n",
      " Training model:  Verb vs not Verb, run 7\n",
      "--Epoch  0  Loss :  0.5217087864875793\n",
      "--Epoch  10  Loss :  0.5476900339126587\n",
      " Training model:  Noun vs Not Noun, run 8\n",
      "--Epoch  0  Loss :  0.5485545992851257\n",
      "--Epoch  10  Loss :  0.5048889517784119\n",
      " Training model:  Adj vs Not Adj, run 8\n",
      "--Epoch  0  Loss :  0.4383688271045685\n",
      "--Epoch  10  Loss :  0.500761866569519\n",
      " Training model:  Verb vs not Verb, run 8\n",
      "--Epoch  0  Loss :  0.5094296336174011\n",
      "--Epoch  10  Loss :  0.3588448762893677\n",
      " Training model:  Noun vs Not Noun, run 9\n",
      "--Epoch  0  Loss :  0.7384288311004639\n",
      "--Epoch  10  Loss :  0.4588499367237091\n",
      " Training model:  Adj vs Not Adj, run 9\n",
      "--Epoch  0  Loss :  0.4383319020271301\n",
      "--Epoch  10  Loss :  0.5632617473602295\n",
      " Training model:  Verb vs not Verb, run 9\n",
      "--Epoch  0  Loss :  0.5517207384109497\n",
      "--Epoch  10  Loss :  0.5459346771240234\n"
     ]
    }
   ],
   "source": [
    "# train the models \n",
    "\n",
    "for y in range(10):\n",
    "    for i in range(3):\n",
    "        print(f\" Training model:  {names[i]}, run {y}\")\n",
    "        \n",
    "        model = nn.Sequential(nn.Linear(512, 2), nn.Softmax(dim=1))\n",
    "        # define the loss function\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        # define the optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        # put the model in training mode\n",
    "        model.train()\n",
    "        for epoch in range(nb_epochs):\n",
    "            for X_train, Y_train in train_loaders[i]:\n",
    "                # compute the model output\n",
    "                Y_pred = model(X_train)\n",
    "                # calculate loss\n",
    "                loss = loss_fn(Y_pred, Y_train)\n",
    "                # reset the gradients\n",
    "                optimizer.zero_grad()\n",
    "                # backpropagation\n",
    "                loss.backward()\n",
    "                # update model weights\n",
    "                optimizer.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"--Epoch \", epoch, \" Loss : \", loss.item())\n",
    "        \n",
    "        weights[i].append(model[0].weight.data.numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.abs(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nouns vs non-Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_weights = pd.DataFrame(columns=list(range(512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_weights['run'] = list(range(10))\n",
    "for r in range(10):\n",
    "    dims_sorted = [x[0] for x in sorted(enumerate(weights[0][r]), key=lambda x: abs(x[1]), reverse=True)]\n",
    "    for i in range(len(dims_sorted)):\n",
    "        noun_weights.iloc[r, dims_sorted[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>328</td>\n",
       "      <td>166</td>\n",
       "      <td>54</td>\n",
       "      <td>187</td>\n",
       "      <td>332</td>\n",
       "      <td>485</td>\n",
       "      <td>149</td>\n",
       "      <td>365</td>\n",
       "      <td>389</td>\n",
       "      <td>151</td>\n",
       "      <td>...</td>\n",
       "      <td>448</td>\n",
       "      <td>83</td>\n",
       "      <td>385</td>\n",
       "      <td>145</td>\n",
       "      <td>475</td>\n",
       "      <td>65</td>\n",
       "      <td>33</td>\n",
       "      <td>222</td>\n",
       "      <td>447</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>225</td>\n",
       "      <td>131</td>\n",
       "      <td>60</td>\n",
       "      <td>190</td>\n",
       "      <td>308</td>\n",
       "      <td>496</td>\n",
       "      <td>194</td>\n",
       "      <td>293</td>\n",
       "      <td>299</td>\n",
       "      <td>132</td>\n",
       "      <td>...</td>\n",
       "      <td>406</td>\n",
       "      <td>100</td>\n",
       "      <td>347</td>\n",
       "      <td>161</td>\n",
       "      <td>463</td>\n",
       "      <td>71</td>\n",
       "      <td>35</td>\n",
       "      <td>200</td>\n",
       "      <td>364</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>278</td>\n",
       "      <td>200</td>\n",
       "      <td>51</td>\n",
       "      <td>238</td>\n",
       "      <td>245</td>\n",
       "      <td>481</td>\n",
       "      <td>172</td>\n",
       "      <td>314</td>\n",
       "      <td>332</td>\n",
       "      <td>178</td>\n",
       "      <td>...</td>\n",
       "      <td>395</td>\n",
       "      <td>85</td>\n",
       "      <td>367</td>\n",
       "      <td>157</td>\n",
       "      <td>463</td>\n",
       "      <td>65</td>\n",
       "      <td>55</td>\n",
       "      <td>210</td>\n",
       "      <td>398</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>348</td>\n",
       "      <td>211</td>\n",
       "      <td>39</td>\n",
       "      <td>265</td>\n",
       "      <td>370</td>\n",
       "      <td>450</td>\n",
       "      <td>217</td>\n",
       "      <td>338</td>\n",
       "      <td>349</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>423</td>\n",
       "      <td>48</td>\n",
       "      <td>311</td>\n",
       "      <td>169</td>\n",
       "      <td>381</td>\n",
       "      <td>85</td>\n",
       "      <td>57</td>\n",
       "      <td>192</td>\n",
       "      <td>494</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>324</td>\n",
       "      <td>150</td>\n",
       "      <td>28</td>\n",
       "      <td>173</td>\n",
       "      <td>302</td>\n",
       "      <td>468</td>\n",
       "      <td>229</td>\n",
       "      <td>310</td>\n",
       "      <td>342</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>450</td>\n",
       "      <td>73</td>\n",
       "      <td>314</td>\n",
       "      <td>167</td>\n",
       "      <td>416</td>\n",
       "      <td>67</td>\n",
       "      <td>50</td>\n",
       "      <td>193</td>\n",
       "      <td>385</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>321</td>\n",
       "      <td>187</td>\n",
       "      <td>26</td>\n",
       "      <td>181</td>\n",
       "      <td>280</td>\n",
       "      <td>445</td>\n",
       "      <td>227</td>\n",
       "      <td>391</td>\n",
       "      <td>322</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>441</td>\n",
       "      <td>62</td>\n",
       "      <td>340</td>\n",
       "      <td>145</td>\n",
       "      <td>448</td>\n",
       "      <td>96</td>\n",
       "      <td>33</td>\n",
       "      <td>213</td>\n",
       "      <td>371</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>316</td>\n",
       "      <td>138</td>\n",
       "      <td>64</td>\n",
       "      <td>229</td>\n",
       "      <td>315</td>\n",
       "      <td>496</td>\n",
       "      <td>164</td>\n",
       "      <td>292</td>\n",
       "      <td>374</td>\n",
       "      <td>130</td>\n",
       "      <td>...</td>\n",
       "      <td>393</td>\n",
       "      <td>55</td>\n",
       "      <td>350</td>\n",
       "      <td>123</td>\n",
       "      <td>429</td>\n",
       "      <td>93</td>\n",
       "      <td>71</td>\n",
       "      <td>223</td>\n",
       "      <td>387</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>324</td>\n",
       "      <td>171</td>\n",
       "      <td>47</td>\n",
       "      <td>235</td>\n",
       "      <td>311</td>\n",
       "      <td>488</td>\n",
       "      <td>158</td>\n",
       "      <td>294</td>\n",
       "      <td>299</td>\n",
       "      <td>166</td>\n",
       "      <td>...</td>\n",
       "      <td>394</td>\n",
       "      <td>53</td>\n",
       "      <td>285</td>\n",
       "      <td>145</td>\n",
       "      <td>472</td>\n",
       "      <td>74</td>\n",
       "      <td>43</td>\n",
       "      <td>268</td>\n",
       "      <td>403</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>348</td>\n",
       "      <td>133</td>\n",
       "      <td>24</td>\n",
       "      <td>235</td>\n",
       "      <td>337</td>\n",
       "      <td>480</td>\n",
       "      <td>201</td>\n",
       "      <td>378</td>\n",
       "      <td>427</td>\n",
       "      <td>146</td>\n",
       "      <td>...</td>\n",
       "      <td>506</td>\n",
       "      <td>80</td>\n",
       "      <td>335</td>\n",
       "      <td>114</td>\n",
       "      <td>471</td>\n",
       "      <td>69</td>\n",
       "      <td>43</td>\n",
       "      <td>215</td>\n",
       "      <td>430</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>356</td>\n",
       "      <td>186</td>\n",
       "      <td>59</td>\n",
       "      <td>264</td>\n",
       "      <td>296</td>\n",
       "      <td>509</td>\n",
       "      <td>232</td>\n",
       "      <td>376</td>\n",
       "      <td>368</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>396</td>\n",
       "      <td>86</td>\n",
       "      <td>339</td>\n",
       "      <td>164</td>\n",
       "      <td>485</td>\n",
       "      <td>67</td>\n",
       "      <td>48</td>\n",
       "      <td>233</td>\n",
       "      <td>453</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1   2    3    4    5    6    7    8    9  ...  503  504  505  506  \\\n",
       "0  328  166  54  187  332  485  149  365  389  151  ...  448   83  385  145   \n",
       "1  225  131  60  190  308  496  194  293  299  132  ...  406  100  347  161   \n",
       "2  278  200  51  238  245  481  172  314  332  178  ...  395   85  367  157   \n",
       "3  348  211  39  265  370  450  217  338  349  170  ...  423   48  311  169   \n",
       "4  324  150  28  173  302  468  229  310  342  120  ...  450   73  314  167   \n",
       "5  321  187  26  181  280  445  227  391  322  170  ...  441   62  340  145   \n",
       "6  316  138  64  229  315  496  164  292  374  130  ...  393   55  350  123   \n",
       "7  324  171  47  235  311  488  158  294  299  166  ...  394   53  285  145   \n",
       "8  348  133  24  235  337  480  201  378  427  146  ...  506   80  335  114   \n",
       "9  356  186  59  264  296  509  232  376  368  158  ...  396   86  339  164   \n",
       "\n",
       "   507 508 509  510  511 run  \n",
       "0  475  65  33  222  447   0  \n",
       "1  463  71  35  200  364   1  \n",
       "2  463  65  55  210  398   2  \n",
       "3  381  85  57  192  494   3  \n",
       "4  416  67  50  193  385   4  \n",
       "5  448  96  33  213  371   5  \n",
       "6  429  93  71  223  387   6  \n",
       "7  472  74  43  268  403   7  \n",
       "8  471  69  43  215  430   8  \n",
       "9  485  67  48  233  453   9  \n",
       "\n",
       "[10 rows x 513 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average ranking of dimensions after 10 runs for **NOUN** vs **non-NOUN**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159     0.4\n",
       "409     0.6\n",
       "305     2.2\n",
       "465     3.8\n",
       "275     3.9\n",
       "378     4.4\n",
       "260     6.1\n",
       "387     8.0\n",
       "462     9.4\n",
       "37     10.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_weights.iloc[:, :512].mean().sort_values()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verb vs non-Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_weights = pd.DataFrame(columns=list(range(512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_weights['run'] = list(range(10))\n",
    "for r in range(10):\n",
    "    dims_sorted = [x[0] for x in sorted(enumerate(weights[2][r]), key=lambda x: abs(x[1]), reverse=True)]\n",
    "    for i in range(len(dims_sorted)):\n",
    "        verb_weights.iloc[r, dims_sorted[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192    0.0\n",
       "310    1.6\n",
       "378    2.0\n",
       "508    4.2\n",
       "480    4.8\n",
       "158    5.0\n",
       "159    6.7\n",
       "175    8.1\n",
       "89     9.1\n",
       "282    9.3\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_weights.iloc[:, :512].mean().sort_values()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adj vs non-Adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_weights = pd.DataFrame(columns=list(range(512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_weights['run'] = list(range(10))\n",
    "for r in range(10):\n",
    "    dims_sorted = [x[0] for x in sorted(enumerate(weights[1][r]), key=lambda x: abs(x[1]), reverse=True)]\n",
    "    for i in range(len(dims_sorted)):\n",
    "        adj_weights.iloc[r, dims_sorted[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256    132.8\n",
       "133    136.1\n",
       "426    136.3\n",
       "98     140.0\n",
       "310    144.5\n",
       "381    144.7\n",
       "12     144.7\n",
       "188    146.1\n",
       "1      146.8\n",
       "412    150.8\n",
       "dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_weights.iloc[:, :512].mean().sort_values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3de7a084b318d7b8bf96005cb5db4da14a27f60df0465391ef48a4c336f03bfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
