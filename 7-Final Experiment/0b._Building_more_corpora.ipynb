{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2957a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Util')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02562512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from we import initiate_model, get_we, create_we_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b49e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44842efb",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27962168",
   "metadata": {},
   "source": [
    "The code will load a Transformer model from Hugging Face library.\n",
    "It is expected that the model can be loaded using `AutoModelForMaskedLM` function of the Hugging Face library and its tokenizer can be initiated using `AutoTokenizer`.\n",
    "\n",
    "This can be confirmed on [the model's page](https://huggingface.co/flaubert/flaubert_base_uncased) in the Hugging Face library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4746fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all models to compare\n",
    "# Since model names can be quite long, label can be used to reference it in the reports. \n",
    "# If label is not present in the model object, its full name will be used in the reports instead.\n",
    "# If label is present in the object, it can't be an empty string.\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        'name': 'xlm-roberta-large',\n",
    "        'label': 'xlm_large'\n",
    "    },\n",
    "    {\n",
    "    \n",
    "        'name': 'xlm-roberta-base', \n",
    "        'label': 'xlm_base'\n",
    "\n",
    "    },\n",
    "    {\n",
    "        'name': 'bert-base-multilingual-uncased',\n",
    "        'label': 'bert_base_u'\n",
    "    },\n",
    "    {\n",
    "        'name': 'distilbert-base-multilingual-cased',\n",
    "        'label': 'distilbert_base'\n",
    "    },\n",
    "    {\n",
    "        'name': 'bert-base-multilingual-cased',\n",
    "        'label': 'bert_base_c'\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca27b55",
   "metadata": {},
   "source": [
    "# Build dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da017a",
   "metadata": {},
   "source": [
    "In our case we build dataset by getting all nouns, verbs and adverbs from Morphalou and their grammatical information.\n",
    "Then we attempt to obtain a word embedding for each of the words. If it exists (meaning that tokenizer tokenizes the word as one token), it's added to the dataset with the following information:\n",
    "- Word \n",
    "- PoS\n",
    "- Grammatical information if present\n",
    "- All dimensions of WE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303064a8",
   "metadata": {},
   "source": [
    "If there is any word that is both masculine and feminine, or other 2 grammatical characteristics at the same time, it was excluded from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838069d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verbs = pd.read_csv('../Data/Morphalou/all_verbs_v2.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e146d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nouns = pd.read_csv('../Data/Morphalou/all_nouns_v2.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6996db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adjs = pd.read_csv('../Data/Morphalou/all_adjs_v2.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae173959",
   "metadata": {},
   "source": [
    "Build the dataset of WE with features for each model in the list above for Verbs, Nouns and Adjectives.\n",
    "\n",
    "Runtime per model is ~1-1.5 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be394fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating model xlm-roberta-base:\n",
      "Done\n",
      "\n",
      "Generating WE for nouns:\n",
      "....................................................................................................................................................................................\n",
      "Nouns WE are being stored in the file: ../Data/xlm_base/all_nouns_we.csv\n",
      "\n",
      "Generating WE for verbs:\n",
      ".................................................................................................................................................................................................................................................................................................................................\n",
      "Verbs WE are being stored in the file: ../Data/xlm_base/all_verbs_we.csv\n",
      "\n",
      "Generating WE for adjs:\n",
      "..................................................................................................\n",
      "Adjs WE are being stored in the file: ../Data/xlm_base/all_adjs_we.csv\n",
      "============================\n",
      "Initiating model bert-base-multilingual-uncased:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15de33ba2ff64e65aad9e1c9c39cd460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcfc151f69045c9a4e60ade75990df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec1747d0a3548c4ae3adccdaa772c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5e18c8bb804de9b2e70e5d474b7411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07170792fb041d4924e052056c4571c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/672M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "\n",
      "Generating WE for nouns:\n",
      "....................................................................................................................................................................................\n",
      "Nouns WE are being stored in the file: ../Data/bert_base_u/all_nouns_we.csv\n",
      "\n",
      "Generating WE for verbs:\n",
      ".................................................................................................................................................................................................................................................................................................................................\n",
      "Verbs WE are being stored in the file: ../Data/bert_base_u/all_verbs_we.csv\n",
      "\n",
      "Generating WE for adjs:\n",
      "..................................................................................................\n",
      "Adjs WE are being stored in the file: ../Data/bert_base_u/all_adjs_we.csv\n",
      "============================\n",
      "Initiating model distilbert-base-multilingual-cased:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5a7be8a1904094b650418f20ffa756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6aac42daa724dfd9d43265e003d5b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8831ee7757a64fddafcde726b6794f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d0e987ff404261953c0b8c53546215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e1d2fe384d4ca6ac92a929d969d3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "\n",
      "Generating WE for nouns:\n",
      "....................................................................................................................................................................................\n",
      "Nouns WE are being stored in the file: ../Data/distilbert_base/all_nouns_we.csv\n",
      "\n",
      "Generating WE for verbs:\n",
      ".................................................................................................................................................................................................................................................................................................................................\n",
      "Verbs WE are being stored in the file: ../Data/distilbert_base/all_verbs_we.csv\n",
      "\n",
      "Generating WE for adjs:\n",
      "..................................................................................................\n",
      "Adjs WE are being stored in the file: ../Data/distilbert_base/all_adjs_we.csv\n",
      "============================\n",
      "Initiating model bert-base-multilingual-cased:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "\n",
      "Generating WE for nouns:\n",
      "....................................................................................................................................................................................\n",
      "Nouns WE are being stored in the file: ../Data/bert_base_c/all_nouns_we.csv\n",
      "\n",
      "Generating WE for verbs:\n",
      ".................................................................................................................................................................................................................................................................................................................................\n",
      "Verbs WE are being stored in the file: ../Data/bert_base_c/all_verbs_we.csv\n",
      "\n",
      "Generating WE for adjs:\n",
      "..................................................................................................\n",
      "Adjs WE are being stored in the file: ../Data/bert_base_c/all_adjs_we.csv\n",
      "============================\n",
      "Initiating model microsoft/mdeberta-v3-base\":\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'microsoft/mdeberta-v3-base\"'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(file_path)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitiating model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m m, t \u001b[38;5;241m=\u001b[39m \u001b[43minitiate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerating WE for nouns:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/WordEmbeddings/5-Comparison of different embeddings/../Util/we.py:17\u001b[0m, in \u001b[0;36minitiate_model\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitiate_model\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflaubert/flaubert_small_cased\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForMaskedLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(name)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# flaubert, log = FlaubertModel.from_pretrained(name, output_loading_info=True)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# flaubert_tokenizer = FlaubertTokenizer.from_pretrained(name, do_lowercase=False)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:643\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    642\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    645\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:487\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03mLoads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03mtokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m    486\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 487\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    414\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[1;32m    108\u001b[0m ):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:164\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have -- or .. in repo_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'microsoft/mdeberta-v3-base\"'."
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    # Get the label of the model if it's present, otherwise get its name.\n",
    "    # Any slashes in the names are replaced with empty strings to work with file saving.\n",
    "    model_label = re.sub('/', '', model.get('label', model['name']))\n",
    "    file_path = f'../Data/{model_label}'\n",
    "    # Create the folder for file storage if it's not created yet.\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "    \n",
    "    print(f'Initiating model {model[\"name\"]}:')\n",
    "    m, t = initiate_model(model['name'])\n",
    "    print('Done')\n",
    "    \n",
    "    print('\\nGenerating WE for nouns:')\n",
    "    nouns_we_df = create_we_df(m, t, all_nouns, progress=True)\n",
    "    print(f'\\nNouns WE are being stored in the file: {file_path}/all_nouns_we.csv')\n",
    "    nouns_we_df.to_csv(f'{file_path}/all_nouns_we.csv')\n",
    "\n",
    "    print('\\nGenerating WE for verbs:')\n",
    "    verbs_we_df = create_we_df(m, t, all_verbs, progress=True)\n",
    "    print(f'\\nVerbs WE are being stored in the file: {file_path}/all_verbs_we.csv')\n",
    "    verbs_we_df.to_csv(f'{file_path}/all_verbs_we.csv')\n",
    "    \n",
    "    print('\\nGenerating WE for adjs:')\n",
    "    adjs_we_df = create_we_df(m, t, all_adjs, progress=True)\n",
    "    print(f'\\nAdjs WE are being stored in the file: {file_path}/all_adjs_we.csv')\n",
    "    adjs_we_df.to_csv(f'{file_path}/all_adjs_we.csv')\n",
    "    \n",
    "    print('============================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c221ef",
   "metadata": {},
   "source": [
    "# NB !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79efb75",
   "metadata": {},
   "source": [
    "Some of the files are too big to be uploaded to Github, all the files can be found in the Google Drive [here](https://drive.google.com/drive/folders/10Ea62GRlq4t7bq-nK9tPtYFu0kbCciey?usp=drive_link)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac720c",
   "metadata": {},
   "source": [
    "# Corpora sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd6f46",
   "metadata": {},
   "source": [
    "Below you will find the sizes of WE datasets for each PoS and for each model: it represents how many unique wordforms have a WE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c16d3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Nouns</th>\n",
       "      <th>Verbs</th>\n",
       "      <th>Adjs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xlm_large</td>\n",
       "      <td>3982</td>\n",
       "      <td>1233</td>\n",
       "      <td>1387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xlm_base</td>\n",
       "      <td>3982</td>\n",
       "      <td>1233</td>\n",
       "      <td>1387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert_base_u</td>\n",
       "      <td>6982</td>\n",
       "      <td>2353</td>\n",
       "      <td>2907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distilbert_base</td>\n",
       "      <td>4494</td>\n",
       "      <td>925</td>\n",
       "      <td>1610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bert_base_c</td>\n",
       "      <td>4494</td>\n",
       "      <td>925</td>\n",
       "      <td>1610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  Nouns  Verbs  Adjs\n",
       "0        xlm_large   3982   1233  1387\n",
       "1         xlm_base   3982   1233  1387\n",
       "2      bert_base_u   6982   2353  2907\n",
       "3  distilbert_base   4494    925  1610\n",
       "4      bert_base_c   4494    925  1610"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes = []\n",
    "\n",
    "for m in models:\n",
    "    model_label = re.sub('/', '', m.get('label', m['name']))\n",
    "    file_path = f'../Data/{model_label}'\n",
    "    \n",
    "    noun_size = len(pd.read_csv(f'{file_path}/all_nouns_we.csv', index_col=0))\n",
    "    verb_size = len(pd.read_csv(f'{file_path}/all_verbs_we.csv', index_col=0))\n",
    "    adj_size = len(pd.read_csv(f'{file_path}/all_adjs_we.csv', index_col=0))\n",
    "    \n",
    "    sizes.append({\n",
    "        'Model': model_label,\n",
    "        'Nouns': noun_size,\n",
    "        'Verbs': verb_size,\n",
    "        'Adjs': adj_size\n",
    "    })\n",
    "    \n",
    "sizes_df = pd.DataFrame(sizes)\n",
    "\n",
    "sizes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d22a46",
   "metadata": {},
   "source": [
    "# A look into the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d73b35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "xlm_model, xlm_tokenizer = initiate_model(models[1]['name'])\n",
    "bc_model, bc_tokenizer = initiate_model(models[-1]['name'])\n",
    "bu_model, bu_tokenizer = initiate_model(models[2]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9daa96b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_models = [models[1]['label'], models[-1]['label'], models[2]['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6b455f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = ['marquante', 'imparfaite', 'déclassement', 'postérieurs', 'absorbent', 'accorda', 'accrédité',\n",
    "             'humilité', 'dures', 'brune', 'reçoit', 'rétroaction', 'niçois', 'inventée', 'cutanées',\n",
    "             'gaieté', 'polémiques', 'approuver', 'rencontrait', 'luttent', 'collent', 'démolir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cdcdbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: marquante\n",
      "    XLM tokenization: ['<s>', 'mar', 'quant', 'e', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'mar', '##quant', '##e', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'mar', '##quant', '##e', '[SEP]']\n",
      "\n",
      "\n",
      "Word: imparfaite\n",
      "    XLM tokenization: ['<s>', 'impar', 'fa', 'ite', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'im', '##par', '##fait', '##e', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'imp', '##ar', '##fait', '##e', '[SEP]']\n",
      "\n",
      "\n",
      "Word: déclassement\n",
      "    XLM tokenization: ['<s>', 'dé', 'class', 'ement', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'dé', '##cla', '##ssement', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'dec', '##lasse', '##ment', '[SEP]']\n",
      "\n",
      "\n",
      "Word: postérieurs\n",
      "    XLM tokenization: ['<s>', 'post', 'érie', 'urs', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'post', '##érieur', '##s', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'poster', '##ieu', '##rs', '[SEP]']\n",
      "\n",
      "\n",
      "Word: absorbent\n",
      "    XLM tokenization: ['<s>', 'absorb', 'ent', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'ab', '##sor', '##bent', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'abs', '##orbe', '##nt', '[SEP]']\n",
      "\n",
      "\n",
      "Word: accorda\n",
      "    XLM tokenization: ['<s>', 'accord', 'a', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'accord', '##a', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'accord', '##a', '[SEP]']\n",
      "\n",
      "\n",
      "Word: accrédité\n",
      "    XLM tokenization: ['<s>', 'ac', 'cré', 'd', 'ité', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'ac', '##c', '##rédit', '##é', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'acc', '##redi', '##te', '[SEP]']\n",
      "\n",
      "\n",
      "Word: humilité\n",
      "    XLM tokenization: ['<s>', 'humil', 'ité', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'hu', '##mil', '##ité', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'hum', '##ili', '##te', '[SEP]']\n",
      "\n",
      "\n",
      "Word: dures\n",
      "    XLM tokenization: ['<s>', 'du', 'res', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'dure', '##s', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'dure', '##s', '[SEP]']\n",
      "\n",
      "\n",
      "Word: brune\n",
      "    XLM tokenization: ['<s>', 'brune', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'brun', '##e', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'brune', '[SEP]']\n",
      "\n",
      "\n",
      "Word: reçoit\n",
      "    XLM tokenization: ['<s>', 're', 'ço', 'it', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'reçoit', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'recoit', '[SEP]']\n",
      "\n",
      "\n",
      "Word: rétroaction\n",
      "    XLM tokenization: ['<s>', 'ré', 'tro', 'action', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'r', '##ét', '##roa', '##ction', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'retro', '##ac', '##tion', '[SEP]']\n",
      "\n",
      "\n",
      "Word: niçois\n",
      "    XLM tokenization: ['<s>', 'ni', 'ço', 'is', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'ni', '##ço', '##is', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'nico', '##is', '[SEP]']\n",
      "\n",
      "\n",
      "Word: inventée\n",
      "    XLM tokenization: ['<s>', 'invent', 'ée', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'in', '##vent', '##ée', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'in', '##vente', '##e', '[SEP]']\n",
      "\n",
      "\n",
      "Word: cutanées\n",
      "    XLM tokenization: ['<s>', 'cu', 'tan', 'ées', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'cut', '##ané', '##es', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'cut', '##ane', '##es', '[SEP]']\n",
      "\n",
      "\n",
      "Word: gaieté\n",
      "    XLM tokenization: ['<s>', 'gai', 'e', 'té', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'gai', '##eté', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'gai', '##ete', '[SEP]']\n",
      "\n",
      "\n",
      "Word: polémiques\n",
      "    XLM tokenization: ['<s>', 'pol', 'ém', 'iques', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'pol', '##émique', '##s', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'pole', '##miques', '[SEP]']\n",
      "\n",
      "\n",
      "Word: approuver\n",
      "    XLM tokenization: ['<s>', 'appro', 'u', 'ver', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'app', '##rou', '##ver', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'app', '##rou', '##ver', '[SEP]']\n",
      "\n",
      "\n",
      "Word: rencontrait\n",
      "    XLM tokenization: ['<s>', 'ren', 'contra', 'it', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'ren', '##contra', '##it', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'ren', '##con', '##tra', '##it', '[SEP]']\n",
      "\n",
      "\n",
      "Word: luttent\n",
      "    XLM tokenization: ['<s>', 'lutte', 'nt', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'lutte', '##nt', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'lutte', '##nt', '[SEP]']\n",
      "\n",
      "\n",
      "Word: collent\n",
      "    XLM tokenization: ['<s>', 'colle', 'nt', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'coll', '##ent', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'colle', '##nt', '[SEP]']\n",
      "\n",
      "\n",
      "Word: démolir\n",
      "    XLM tokenization: ['<s>', 'dé', 'moli', 'r', '</s>']\n",
      "    BERT-cased tokenization: ['[CLS]', 'dé', '##mol', '##ir', '[SEP]']\n",
      "    BERT-uncased tokenization: ['[CLS]', 'demo', '##lir', '[SEP]']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in test_words:\n",
    "    print(f\"\"\"Word: {w}\n",
    "    XLM tokenization: {[xlm_tokenizer.decode(x) for x in xlm_tokenizer.encode(w)]}\n",
    "    BERT-cased tokenization: {[bc_tokenizer.decode(x) for x in bc_tokenizer.encode(w)]}\n",
    "    BERT-uncased tokenization: {[bu_tokenizer.decode(x) for x in bu_tokenizer.encode(w)]}\\n\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5efc898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = initiate_model(models[-3]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e642a0",
   "metadata": {},
   "source": [
    "# Creating unique WE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820de15",
   "metadata": {},
   "source": [
    "Some words can be interpretted as several parts of speech so to avoid the ambiguity, we will exlclude these words and we'll only keep words uniquely interpreted as one PoS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efc1afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(models)):\n",
    "\n",
    "    nouns_df = pd.read_csv(f'../Data/{models[i][\"label\"]}/all_nouns_we.csv', index_col=0)\n",
    "    nouns_df['POS'] = 'NOUN'\n",
    "\n",
    "    verbs_df = pd.read_csv(f'../Data/{models[i][\"label\"]}/all_verbs_we.csv', index_col=0)\n",
    "    verbs_df['POS'] = 'VERB'\n",
    "\n",
    "    adjs_df = pd.read_csv(f'../Data/{models[i][\"label\"]}/all_adjs_we.csv', index_col=0)\n",
    "    adjs_df['POS'] = 'ADJ'\n",
    "\n",
    "    combined_df = pd.concat([nouns_df, verbs_df, adjs_df])\n",
    "    words, counts = np.unique(combined_df.index, return_counts=True)\n",
    "    unique_words = [x[0] for x in zip(words, counts) if x[1] == 1]\n",
    "    combined_df = combined_df[combined_df.index.isin(unique_words)]\n",
    "\n",
    "    combined_df.to_csv(f'../Data/{models[i][\"label\"]}/all_unique_pos_we.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
