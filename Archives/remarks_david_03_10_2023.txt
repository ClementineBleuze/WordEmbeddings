+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
pos_correlation_experiment_v1.ipynb
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

"For one isoldated feature, we try to isolate dimensions that seem to encode it. This is made by computing average values of WE among classes (e.g)"

==> Actually I do not think it is the correct method. If I understand well, for each WE component x, you compute the average value of x for NOUNs and for VERBs, and you compare the both values. The more they differ, the more x encode POS (NOUN vs VERB). But this implies that there is ONE average. Maybe x encode sevral information: NOUN_gender_number, VERB_gender_number and then, there are several averages for x, or there several interval (for example [-1.. -0.75] = NOUN_m_p, ]-0.75..-0.5] = VERB_m_s, ]-0.5..-0.25] = NOUN_m_p, ]-0.25..-0] = VERB_m_p, and so on (values for VERBs and NOUNs are interlaced).

How did you extract most frequent NOUNs and VERBs? From which source?

I see that for "chose", the frequency is 1321.79. Frequency is an integer, isn't it? What is frequency in this file?

chose, temps, vie, fois, peu are the most frequent NOUNs. This is surprising, but, the webpage https://blog.lingoda.com/fr/les-mots-les-plus-utilises-en-francais/ gives: homme, mari, femme, jour, mer, temps, main, chose, vie, yeux. So, why not... What is the source for frequency?

Frequency dimension : no WE feature correlated with Frequency. This is not surprising for me because I do not see why NOUNs with closed frequencies could have smae behaviors in the corpus. But If you include all words, maybe you could find a correlation because most frequent words are ARTICLEs for example.

Dimensions with highest and lowest avg
======================================

"We can make an assumption that dimensions that have highest or lowest values can be representative of noun-ness." --> Why?
For WE feature x, you could have  a segmentation of values into intervals for sevral kinds of NOUNs (one interval for each NOUN_g_n, g in {masculine, feminine}, n in {singular, plural}). Then, all values indicate NOUN, and maybe feature x should not be read if the word is not a NOUN.
BUT, this part of work is in the case "one WE feature = one linguistic feature".

"We can notice that all values of dimension 181 are negative and we can hypothesise that positive values of this dimension can correspond to non-nouns." : it is easy to check.

About the average : actually you can not conclude because you use only NOUNs. If you want to check your hypothesis, you have to compute the average for non-NOUNs and compare.


unique_freq_n_we['326'].sort_values()[-20:]: the main part of words are linked to 'clothes world'. Surprising. A comment?


Dimensions with the lowest std
==============================

"We can create a hypithesis that these dimensions with the lowest deviation, represent the noun-ness.": yes, maybe.
You don't harvest this hypothesis?

Testing the correlation of noun-ness
====================================
Yes, the protocol answers to my previous remark.

About feature 508. OK, now you could study the distribution of NOUNs and non-NOUNs for 508.
I propose to plot all word from the minimum value of 508 ot the maximum value of 508.
Where are NOUNs ? Can we find other sectors for other kinds of words?
List
POS word frequency
from lowest to highest of 508. What do you see?


Finding dimensions with highest correlation to PoS values
=========================================================
OK, seek previous remark about 508for the new detected WE features

WE mean and WE std
==================

"We can hypothesise that not only a dimension value can be correspondent to a PoS information but a mean value of all dimensions of a WE can be holding a connection to its PoS: since potentially multiple dimensions are correspondant to PoS. And similarly, the row's std could be related to the PoS as well."
--> Actually, I guess that a subpart of a vector indicates one information. The problem is to find where is this subpart.

====> we use a lot the VALUE of features. But actually this value has no sense, this value indicates only a position on one axis. Maybe we should study relative position of several classes ( NOUN/VERB/ADJECTICE/etc, NOUM_f_p/NOUN_f_s/NOUN_m_p/NOUN_m_s on the different axis...

I am not sure that dealing with the entire WE is a good direction for work, because you mix a lot of information in ony one value.


Constructing an average vector for nouns and verbs
==================================================

This protocol corresponds to what I proposed: comparing WE features across several POS.

!!!!! You should normalize the difference for a dimension by taking account the difference between maxximum and minimum value for this dimension. Unless you can not compare differences.


Investigating the dimensions
============================

"Before proceeding we can have a look at the correlation of the dimensions between themselves and with PoS information:"
--> Attention: you focus only on verbs and nouns, not on all POS.

Question: are we sure that correlation is a good measure to check the link between continuous values and 0/1?

sns.histplot(data=unique_n_and_v_we, x='29', hue='cgram') : what are the gray bars? --> OK, I understand this is overlap --> the graph is very easy to read!

try to focus only of frequent nouns and verbs to check if the overlap decreases.

"However, there is no common pattern for all these nouns.": add frequency information.
==> "unique_freq_v_we" sorry, you use yet most fequent versb and nouns... What's the matter if you take the "very most" frequent?

"The verbs being transitive => Tu le fais and je le mange being interpretted as a noun" : you try to analyse outliers with linguistic information. But, if your hypothesis is feature x = POS, then other linguistic information should not be used as explanation. The overlap problem should be harvested differently. How? I propose now only frequency...

Idea: participes passés can be used as noun : "tu prends le *cassé* et tu le jettes". Maybe this why we retrieve a lot of participes passés in verbs outliers. Maybe it is what you want to say by "We can see that both typical and outlier verbs include participe passé forms, making it hard to distinguish them."

"Interestingly, the typical verbs are all in 2nd person plural. This information together with the outlier nouns that are used for address: it could be a sign of a more formal context where these words are used." --> yes interesting. Keep this feature in mind with this remark


It seems that outliers are frequently the same words for several features. Right?



unique_freq_n_we.loc[:, '480'].sort_values()[:20] : it should be verbs, shouldn't it?




Results
=======

Adding adjectives and adverbs
=============================

top_freq_adj = pd.read_csv('Most Frequent Words/freq_ADJ.csv', index_col=0)[:1000]
---> mon, ce, ma, cette, votre are not ADJ but PRONOUNS

"We can see that there is a quite big overlap in values of the dimension with the highest correlation to PoS"
--> Yes, it becomes difficult to say that ONE feature code the linguistic features POS.



Conclusion
==========

"Since the PoS information doesn't seem to be directly corresponding to any of the dimension, we can assume that PoS is retrieved from co-occurence information: e.g. nouns co-occuring with articles, verbs co-occuring with pronouns like je/il/..."
--> Or the POS is coded by several WE features.
There is one WE per word. The co-occurrences led to the WE. Therefore I think that POS information is somewhere in WE.
This great work allows to conclude that there is "something". We have to find where.

"We have observed that for several of the dimensions with the highest correlation to PoS information, mean for values of the dimension for verbs is relatively close to 0: 159, 212, 51, 198 which can be potentially interesting to investigate: the closeness of dimension values to 0 on average."
see my remark about: value of WE feature = position. I think it can not be read as weigth of the affirmation "it is a NOUN" opr something else.


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
retrieving-gender-information.ipynb
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Here also, to compare differences, you must divde by the différence between max and min for each feature.
Please rebuild results with this normalisation.

371: about very high std, please explore what's the matter with 371.

Distribution for dimensions 508, 158, 321 [m,f] : and what about 371?

"So these 7 dimensions not only have intra-classes low std, but also extra-class low std. We reject the hypothesis that they encode gender. However, maybe they encode noun-ness."
--> Yes compaed to POS, the overlap is very big.
Maybe we will have to deal with more fine-grained classe: POS_gender_number_tense_etc

"Although no clear statement can be done, it seems that low values on dim 100 correlate with words depicting animal/mineral/beings/food or at least physical and concrete beings and artefacts (bambou, chêne, dinosaures, requin, noisettes, araignées) - but this is more visible in masculine nouns."

--> I think that the most important part of this sentence is "Although no clear statement can be done" ;-)

Finally my proposition are the same than for POS:
- normalise when it is necessary
- increase the threshold of the "is_frequent" in order to check if the overlap decreases when threshold increases


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
retriving_nomber_noun_information.ipynb
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

plt.plot(range(len(sg_n_we)), sg_n_we.loc[:, '418'], label='Singular Noun')
plt.plot(range(len(sg_n_we)), pl_n_we.loc[:, '418'][:len(sg_n_we)], label='Plural Noun')
plt.xlabel('Word index')
plt.ylabel('Dimension 418 value')
plt.legend()

--> instead of the grapĥ, I think that histograms are more clear to read

sg_pl_n_we.loc[:, '215'].sort_values()[-10:]
--> surprisingly, you have singular/plural flexions of the same word withvery close values.
--> so 215 does not code number, does it?

sg_pl_n_we.loc[:, '215'].sort_values()[:10]
and here, you have words for the body...
surprising because you focus on WE features more correlated with number...

sg_pl_n_we.loc[:, '453'].sort_values()[:10]
--> fin occurs twice.

And I see that we find sometimes the same words in max/min values of different WE features.
Mabe this because sevaral of these features code number.

"We see that dimensions 310, 54, 288, 278, 172 have a stronger correlation beetween them, and for 418, 215, 453, 14, 51, they are neither highly correlated with other dimensions nor highly correlated with each other.

From the above results we can conclude that maybe one or several of these vectors ： 310， 54， 288， 278， 172 encode the information of Singular and Plural Noun.
"
--> Yes, you can now combine these features.












