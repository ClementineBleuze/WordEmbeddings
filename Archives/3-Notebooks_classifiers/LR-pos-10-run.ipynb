{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data: NOUN, ADJ, and both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = pd.read_csv('../Data/FlauBERT_WE/all_nouns_we.csv', index_col=0).drop(columns=['gender', 'number'])\n",
    "nouns['noun'] = 1\n",
    "nouns['verb'] = 0\n",
    "nouns['adj'] = 0\n",
    "\n",
    "verbs = pd.read_csv('../Data/FlauBERT_WE/all_verb_we.csv', index_col=0)\n",
    "verbs['noun'] = 0\n",
    "verbs['verb'] = 1\n",
    "verbs['adj'] = 0\n",
    "\n",
    "adjs = pd.read_csv('../Data/FlauBERT_WE/all_adjectives_we.csv', index_col=0).drop(columns=['gender', 'number'])\n",
    "adjs['noun'] = 0\n",
    "adjs['verb'] = 0\n",
    "adjs['adj'] = 1\n",
    "\n",
    "\n",
    "data = pd.concat([nouns, adjs, verbs])\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "normalized_data = (data - data.min())/(data.max() - data.min()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target : gender\n",
    "Y_N = np.asarray(normalized_data.noun)\n",
    "Y_V = np.asarray(normalized_data.verb)\n",
    "Y_A = np.asarray(normalized_data.adj)\n",
    "\n",
    "# features : word embeddings dimensions\n",
    "X = np.asarray(normalized_data.iloc[:, :512])\n",
    "\n",
    "# split data into train and test sets\n",
    "X_N_train, X_N_test, Y_N_train, Y_N_test = train_test_split(X, Y_N, test_size=0.2, random_state=42)\n",
    "X_A_train, X_A_test, Y_A_train, Y_A_test = train_test_split(X, Y_A, test_size=0.2, random_state=42)\n",
    "X_V_train, X_V_test, Y_V_train, Y_V_test = train_test_split(X, Y_V, test_size=0.2, random_state=42)\n",
    "\n",
    "names = ['Noun vs Not Noun', 'Adj vs Not Adj', 'Verb vs not Verb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = [X_N_train, X_A_train, X_V_train]\n",
    "test_features = [X_N_test, X_A_test, X_V_test]\n",
    "train_targets = [Y_N_train, Y_A_train, Y_V_train]\n",
    "test_targets = [Y_N_test, Y_A_test, Y_V_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [[], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training model:  Noun vs Not Noun, run 0\n",
      " Training model:  Adj vs Not Adj, run 0\n",
      " Training model:  Verb vs not Verb, run 0\n",
      " Training model:  Noun vs Not Noun, run 1\n",
      " Training model:  Adj vs Not Adj, run 1\n",
      " Training model:  Verb vs not Verb, run 1\n",
      " Training model:  Noun vs Not Noun, run 2\n",
      " Training model:  Adj vs Not Adj, run 2\n",
      " Training model:  Verb vs not Verb, run 2\n",
      " Training model:  Noun vs Not Noun, run 3\n",
      " Training model:  Adj vs Not Adj, run 3\n",
      " Training model:  Verb vs not Verb, run 3\n",
      " Training model:  Noun vs Not Noun, run 4\n",
      " Training model:  Adj vs Not Adj, run 4\n",
      " Training model:  Verb vs not Verb, run 4\n",
      " Training model:  Noun vs Not Noun, run 5\n",
      " Training model:  Adj vs Not Adj, run 5\n",
      " Training model:  Verb vs not Verb, run 5\n",
      " Training model:  Noun vs Not Noun, run 6\n",
      " Training model:  Adj vs Not Adj, run 6\n",
      " Training model:  Verb vs not Verb, run 6\n",
      " Training model:  Noun vs Not Noun, run 7\n",
      " Training model:  Adj vs Not Adj, run 7\n",
      " Training model:  Verb vs not Verb, run 7\n",
      " Training model:  Noun vs Not Noun, run 8\n",
      " Training model:  Adj vs Not Adj, run 8\n",
      " Training model:  Verb vs not Verb, run 8\n",
      " Training model:  Noun vs Not Noun, run 9\n",
      " Training model:  Adj vs Not Adj, run 9\n",
      " Training model:  Verb vs not Verb, run 9\n"
     ]
    }
   ],
   "source": [
    "# train the models \n",
    "\n",
    "for y in range(10):\n",
    "    for i in range(3):\n",
    "        print(f\" Training model:  {names[i]}, run {y}\")\n",
    "        \n",
    "        clf = LogisticRegression(random_state=y, max_iter=1000)\n",
    "        clf.fit(train_features[i], train_targets[i])\n",
    "        \n",
    "        weights[i].append(clf.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.abs(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nouns vs non-Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_weights = pd.DataFrame(columns=list(range(512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_weights['run'] = list(range(10))\n",
    "for r in range(10):\n",
    "    dims_sorted = [x[0] for x in sorted(enumerate(weights[0][r]), key=lambda x: abs(x[1]), reverse=True)]\n",
    "    for i in range(len(dims_sorted)):\n",
    "        noun_weights.iloc[r, dims_sorted[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average ranking of dimensions after 10 runs for **NOUN** vs **non-NOUN**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52     0.0\n",
       "92     1.0\n",
       "261    2.0\n",
       "275    3.0\n",
       "229    4.0\n",
       "427    5.0\n",
       "132    6.0\n",
       "37     7.0\n",
       "345    8.0\n",
       "223    9.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_weights.iloc[:, :512].mean().sort_values()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verb vs non-Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_weights = pd.DataFrame(columns=list(range(512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_weights['run'] = list(range(10))\n",
    "for r in range(10):\n",
    "    dims_sorted = [x[0] for x in sorted(enumerate(weights[2][r]), key=lambda x: abs(x[1]), reverse=True)]\n",
    "    for i in range(len(dims_sorted)):\n",
    "        verb_weights.iloc[r, dims_sorted[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12     0.0\n",
       "261    1.0\n",
       "216    2.0\n",
       "192    3.0\n",
       "291    4.0\n",
       "310    5.0\n",
       "341    6.0\n",
       "92     7.0\n",
       "275    8.0\n",
       "56     9.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_weights.iloc[:, :512].mean().sort_values()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adj vs non-Adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_weights = pd.DataFrame(columns=list(range(512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_weights['run'] = list(range(10))\n",
    "for r in range(10):\n",
    "    dims_sorted = [x[0] for x in sorted(enumerate(weights[1][r]), key=lambda x: abs(x[1]), reverse=True)]\n",
    "    for i in range(len(dims_sorted)):\n",
    "        adj_weights.iloc[r, dims_sorted[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133    0.0\n",
       "328    1.0\n",
       "81     2.0\n",
       "21     3.0\n",
       "310    4.0\n",
       "292    5.0\n",
       "260    6.0\n",
       "26     7.0\n",
       "369    8.0\n",
       "110    9.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_weights.iloc[:, :512].mean().sort_values()[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3de7a084b318d7b8bf96005cb5db4da14a27f60df0465391ef48a4c336f03bfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
