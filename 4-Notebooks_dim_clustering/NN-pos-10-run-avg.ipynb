{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Classification (Perceptron) : Noun/ Not-Noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data: NOUN, ADJ, and both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = pd.read_csv('../Data/FlauBERT_WE/all_nouns_we.csv', index_col=0).drop(columns=['gender', 'number'])\n",
    "nouns['noun'] = 1\n",
    "nouns['verb'] = 0\n",
    "nouns['adj'] = 0\n",
    "\n",
    "verbs = pd.read_csv('../Data/FlauBERT_WE/all_verb_we.csv', index_col=0)\n",
    "verbs['noun'] = 0\n",
    "verbs['verb'] = 1\n",
    "verbs['adj'] = 0\n",
    "\n",
    "adjs = pd.read_csv('../Data/FlauBERT_WE/all_adjectives_we.csv', index_col=0).drop(columns=['gender', 'number'])\n",
    "adjs['noun'] = 0\n",
    "adjs['verb'] = 0\n",
    "adjs['adj'] = 1\n",
    "\n",
    "\n",
    "data = pd.concat([nouns, adjs, verbs])\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "normalized_data = (data - data.min())/(data.max() - data.min()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target : gender\n",
    "Y_N = np.asarray(normalized_data.noun)\n",
    "Y_V = np.asarray(normalized_data.verb)\n",
    "Y_A = np.asarray(normalized_data.adj)\n",
    "\n",
    "# features : word embeddings dimensions\n",
    "X = np.asarray(normalized_data.iloc[:, :512])\n",
    "\n",
    "# split data into train and test sets\n",
    "X_N_train, X_N_test, Y_N_train, Y_N_test = train_test_split(X, Y_N, test_size=0.2, random_state=42)\n",
    "X_A_train, X_A_test, Y_A_train, Y_A_test = train_test_split(X, Y_A, test_size=0.2, random_state=42)\n",
    "X_V_train, X_V_test, Y_V_train, Y_V_test = train_test_split(X, Y_V, test_size=0.2, random_state=42)\n",
    "\n",
    "names = ['Noun vs Not Noun', 'Adj vs Not Adj', 'Verb vs not Verb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = [X_N_train, X_A_train, X_V_train]\n",
    "test_features = [X_N_test, X_A_test, X_V_test]\n",
    "train_targets = [Y_N_train, Y_A_train, Y_V_train]\n",
    "test_targets = [Y_N_test, Y_A_test, Y_V_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensors\n",
    "train_features = [torch.tensor(x).float() for x in train_features]\n",
    "test_features = [torch.tensor(x).float() for x in test_features]\n",
    "train_targets = [torch.tensor(x).long() for x in train_targets]\n",
    "test_targets = [torch.tensor(x).long() for x in test_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# the TensorDataset is a ready to use class to represent your data as list of tensors. \n",
    "# Note that input_features and labels must match on the length of the first dimension\n",
    "train_sets = [TensorDataset(X_train, Y_train) for X_train, Y_train in zip(train_features, train_targets)]\n",
    "test_sets = [TensorDataset(X_valid, Y_valid) for X_valid, Y_valid in zip(test_features, test_targets)]\n",
    "\n",
    "# DataLoader shuffles and batches the data and load its in parallel using multiprocessing workers\n",
    "train_loaders = [DataLoader(train_set, batch_size=32, shuffle=True) for train_set in train_sets]\n",
    "test_loaders = [DataLoader(test_set, batch_size=32) for test_set in test_sets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNlist = []\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    # create a fully connected perceptron with 1 input layer (512 features) and 1 output layer (2 classes)\n",
    "    model = nn.Sequential(nn.Linear(512, 2), nn.Softmax(dim=1))\n",
    "    # define the loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # add the model to the list\n",
    "    NNlist.append([model, loss_fn, optimizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [[], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training model:  Noun vs Not Noun, run 0\n",
      "--Epoch  0  Loss :  0.5869765877723694\n",
      "--Epoch  10  Loss :  0.6973314881324768\n",
      " Training model:  Adj vs Not Adj, run 0\n",
      "--Epoch  0  Loss :  0.6257501244544983\n",
      "--Epoch  10  Loss :  0.5632616877555847\n",
      " Training model:  Verb vs not Verb, run 0\n",
      "--Epoch  0  Loss :  0.6199825406074524\n",
      "--Epoch  10  Loss :  0.5220359563827515\n",
      " Training model:  Noun vs Not Noun, run 1\n",
      "--Epoch  0  Loss :  0.5855097770690918\n",
      "--Epoch  10  Loss :  0.5742368102073669\n",
      " Training model:  Adj vs Not Adj, run 1\n",
      "--Epoch  0  Loss :  0.5008059740066528\n",
      "--Epoch  10  Loss :  0.5632617473602295\n",
      " Training model:  Verb vs not Verb, run 1\n",
      "--Epoch  0  Loss :  0.6043552756309509\n",
      "--Epoch  10  Loss :  0.4314430356025696\n",
      " Training model:  Noun vs Not Noun, run 2\n",
      "--Epoch  0  Loss :  0.7129372954368591\n",
      "--Epoch  10  Loss :  0.5634873509407043\n",
      " Training model:  Adj vs Not Adj, run 2\n",
      "--Epoch  0  Loss :  0.5632551908493042\n",
      "--Epoch  10  Loss :  0.37576207518577576\n",
      " Training model:  Verb vs not Verb, run 2\n",
      "--Epoch  0  Loss :  0.6728284358978271\n",
      "--Epoch  10  Loss :  0.5003045201301575\n",
      " Training model:  Noun vs Not Noun, run 3\n",
      "--Epoch  0  Loss :  0.6560068130493164\n",
      "--Epoch  10  Loss :  0.6350562572479248\n",
      " Training model:  Adj vs Not Adj, run 3\n",
      "--Epoch  0  Loss :  0.500783383846283\n",
      "--Epoch  10  Loss :  0.5632617473602295\n",
      " Training model:  Verb vs not Verb, run 3\n",
      "--Epoch  0  Loss :  0.7844514846801758\n",
      "--Epoch  10  Loss :  0.4000271260738373\n",
      " Training model:  Noun vs Not Noun, run 4\n",
      "--Epoch  0  Loss :  0.5284576416015625\n",
      "--Epoch  10  Loss :  0.5498329997062683\n",
      " Training model:  Adj vs Not Adj, run 4\n",
      "--Epoch  0  Loss :  0.5007973909378052\n",
      "--Epoch  10  Loss :  0.3757622241973877\n",
      " Training model:  Verb vs not Verb, run 4\n",
      "--Epoch  0  Loss :  0.6154873967170715\n",
      "--Epoch  10  Loss :  0.40571242570877075\n",
      " Training model:  Noun vs Not Noun, run 5\n",
      "--Epoch  0  Loss :  0.5879117846488953\n",
      "--Epoch  10  Loss :  0.4877825379371643\n",
      " Training model:  Adj vs Not Adj, run 5\n",
      "--Epoch  0  Loss :  0.6257520914077759\n",
      "--Epoch  10  Loss :  0.6882615685462952\n",
      " Training model:  Verb vs not Verb, run 5\n",
      "--Epoch  0  Loss :  0.592386782169342\n",
      "--Epoch  10  Loss :  0.5911474823951721\n",
      " Training model:  Noun vs Not Noun, run 6\n",
      "--Epoch  0  Loss :  0.6192887425422668\n",
      "--Epoch  10  Loss :  0.5119963884353638\n",
      " Training model:  Adj vs Not Adj, run 6\n",
      "--Epoch  0  Loss :  0.43834325671195984\n",
      "--Epoch  10  Loss :  0.37576231360435486\n",
      " Training model:  Verb vs not Verb, run 6\n",
      "--Epoch  0  Loss :  0.559357762336731\n",
      "--Epoch  10  Loss :  0.34575963020324707\n",
      " Training model:  Noun vs Not Noun, run 7\n",
      "--Epoch  0  Loss :  0.550411581993103\n",
      "--Epoch  10  Loss :  0.49331411719322205\n",
      " Training model:  Adj vs Not Adj, run 7\n",
      "--Epoch  0  Loss :  0.4383355677127838\n",
      "--Epoch  10  Loss :  0.5632616281509399\n",
      " Training model:  Verb vs not Verb, run 7\n",
      "--Epoch  0  Loss :  0.5765758156776428\n",
      "--Epoch  10  Loss :  0.6036957502365112\n",
      " Training model:  Noun vs Not Noun, run 8\n",
      "--Epoch  0  Loss :  0.5740022659301758\n",
      "--Epoch  10  Loss :  0.678837239742279\n",
      " Training model:  Adj vs Not Adj, run 8\n",
      "--Epoch  0  Loss :  0.43829378485679626\n",
      "--Epoch  10  Loss :  0.3757619559764862\n",
      " Training model:  Verb vs not Verb, run 8\n",
      "--Epoch  0  Loss :  0.6147946119308472\n",
      "--Epoch  10  Loss :  0.5894332528114319\n",
      " Training model:  Noun vs Not Noun, run 9\n",
      "--Epoch  0  Loss :  0.5791451334953308\n",
      "--Epoch  10  Loss :  0.5612688660621643\n",
      " Training model:  Adj vs Not Adj, run 9\n",
      "--Epoch  0  Loss :  0.5007907152175903\n",
      "--Epoch  10  Loss :  0.6257615685462952\n",
      " Training model:  Verb vs not Verb, run 9\n",
      "--Epoch  0  Loss :  0.611102819442749\n",
      "--Epoch  10  Loss :  0.4538688659667969\n"
     ]
    }
   ],
   "source": [
    "# train the models \n",
    "\n",
    "for y in range(10):\n",
    "    for i in range(3):\n",
    "        print(f\" Training model:  {names[i]}, run {y}\")\n",
    "        \n",
    "        model = nn.Sequential(nn.Linear(512, 2), nn.Softmax(dim=1))\n",
    "        # define the loss function\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        # define the optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        # put the model in training mode\n",
    "        model.train()\n",
    "        for epoch in range(nb_epochs):\n",
    "            for X_train, Y_train in train_loaders[i]:\n",
    "                # compute the model output\n",
    "                Y_pred = model(X_train)\n",
    "                # calculate loss\n",
    "                loss = loss_fn(Y_pred, Y_train)\n",
    "                # reset the gradients\n",
    "                optimizer.zero_grad()\n",
    "                # backpropagation\n",
    "                loss.backward()\n",
    "                # update model weights\n",
    "                optimizer.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"--Epoch \", epoch, \" Loss : \", loss.item())\n",
    "        \n",
    "        weights[i].append(model[0].weight.data.numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.abs(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun dimensions ranking after 10 runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_weights = pd.DataFrame(columns=list(range(512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_weights['run'] = list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(10):\n",
    "    dims_sorted = np.argsort(weights[0][r])\n",
    "    for i in range(len(dims_sorted)):\n",
    "        noun_weights.iloc[r, dims_sorted[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>227</td>\n",
       "      <td>396</td>\n",
       "      <td>478</td>\n",
       "      <td>198</td>\n",
       "      <td>185</td>\n",
       "      <td>63</td>\n",
       "      <td>345</td>\n",
       "      <td>305</td>\n",
       "      <td>42</td>\n",
       "      <td>406</td>\n",
       "      <td>...</td>\n",
       "      <td>132</td>\n",
       "      <td>434</td>\n",
       "      <td>21</td>\n",
       "      <td>317</td>\n",
       "      <td>78</td>\n",
       "      <td>452</td>\n",
       "      <td>474</td>\n",
       "      <td>140</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158</td>\n",
       "      <td>366</td>\n",
       "      <td>460</td>\n",
       "      <td>202</td>\n",
       "      <td>231</td>\n",
       "      <td>38</td>\n",
       "      <td>356</td>\n",
       "      <td>277</td>\n",
       "      <td>109</td>\n",
       "      <td>397</td>\n",
       "      <td>...</td>\n",
       "      <td>133</td>\n",
       "      <td>430</td>\n",
       "      <td>33</td>\n",
       "      <td>296</td>\n",
       "      <td>92</td>\n",
       "      <td>412</td>\n",
       "      <td>464</td>\n",
       "      <td>136</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>230</td>\n",
       "      <td>381</td>\n",
       "      <td>453</td>\n",
       "      <td>261</td>\n",
       "      <td>195</td>\n",
       "      <td>9</td>\n",
       "      <td>311</td>\n",
       "      <td>292</td>\n",
       "      <td>80</td>\n",
       "      <td>383</td>\n",
       "      <td>...</td>\n",
       "      <td>97</td>\n",
       "      <td>437</td>\n",
       "      <td>15</td>\n",
       "      <td>320</td>\n",
       "      <td>87</td>\n",
       "      <td>424</td>\n",
       "      <td>467</td>\n",
       "      <td>212</td>\n",
       "      <td>170</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>207</td>\n",
       "      <td>342</td>\n",
       "      <td>480</td>\n",
       "      <td>246</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>321</td>\n",
       "      <td>261</td>\n",
       "      <td>112</td>\n",
       "      <td>381</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>434</td>\n",
       "      <td>18</td>\n",
       "      <td>341</td>\n",
       "      <td>123</td>\n",
       "      <td>435</td>\n",
       "      <td>478</td>\n",
       "      <td>237</td>\n",
       "      <td>86</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>208</td>\n",
       "      <td>347</td>\n",
       "      <td>487</td>\n",
       "      <td>299</td>\n",
       "      <td>213</td>\n",
       "      <td>5</td>\n",
       "      <td>324</td>\n",
       "      <td>327</td>\n",
       "      <td>54</td>\n",
       "      <td>374</td>\n",
       "      <td>...</td>\n",
       "      <td>167</td>\n",
       "      <td>437</td>\n",
       "      <td>89</td>\n",
       "      <td>355</td>\n",
       "      <td>28</td>\n",
       "      <td>411</td>\n",
       "      <td>456</td>\n",
       "      <td>191</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>142</td>\n",
       "      <td>382</td>\n",
       "      <td>467</td>\n",
       "      <td>173</td>\n",
       "      <td>167</td>\n",
       "      <td>64</td>\n",
       "      <td>337</td>\n",
       "      <td>236</td>\n",
       "      <td>103</td>\n",
       "      <td>388</td>\n",
       "      <td>...</td>\n",
       "      <td>116</td>\n",
       "      <td>417</td>\n",
       "      <td>32</td>\n",
       "      <td>314</td>\n",
       "      <td>106</td>\n",
       "      <td>428</td>\n",
       "      <td>472</td>\n",
       "      <td>219</td>\n",
       "      <td>71</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>143</td>\n",
       "      <td>376</td>\n",
       "      <td>460</td>\n",
       "      <td>257</td>\n",
       "      <td>224</td>\n",
       "      <td>17</td>\n",
       "      <td>382</td>\n",
       "      <td>286</td>\n",
       "      <td>64</td>\n",
       "      <td>398</td>\n",
       "      <td>...</td>\n",
       "      <td>95</td>\n",
       "      <td>436</td>\n",
       "      <td>102</td>\n",
       "      <td>342</td>\n",
       "      <td>110</td>\n",
       "      <td>453</td>\n",
       "      <td>466</td>\n",
       "      <td>163</td>\n",
       "      <td>150</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>176</td>\n",
       "      <td>344</td>\n",
       "      <td>482</td>\n",
       "      <td>255</td>\n",
       "      <td>230</td>\n",
       "      <td>3</td>\n",
       "      <td>313</td>\n",
       "      <td>267</td>\n",
       "      <td>159</td>\n",
       "      <td>380</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>443</td>\n",
       "      <td>62</td>\n",
       "      <td>339</td>\n",
       "      <td>65</td>\n",
       "      <td>421</td>\n",
       "      <td>468</td>\n",
       "      <td>157</td>\n",
       "      <td>168</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>207</td>\n",
       "      <td>340</td>\n",
       "      <td>457</td>\n",
       "      <td>286</td>\n",
       "      <td>225</td>\n",
       "      <td>48</td>\n",
       "      <td>373</td>\n",
       "      <td>212</td>\n",
       "      <td>59</td>\n",
       "      <td>395</td>\n",
       "      <td>...</td>\n",
       "      <td>131</td>\n",
       "      <td>406</td>\n",
       "      <td>55</td>\n",
       "      <td>362</td>\n",
       "      <td>122</td>\n",
       "      <td>456</td>\n",
       "      <td>435</td>\n",
       "      <td>241</td>\n",
       "      <td>127</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>119</td>\n",
       "      <td>368</td>\n",
       "      <td>478</td>\n",
       "      <td>257</td>\n",
       "      <td>176</td>\n",
       "      <td>87</td>\n",
       "      <td>382</td>\n",
       "      <td>311</td>\n",
       "      <td>90</td>\n",
       "      <td>355</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>444</td>\n",
       "      <td>17</td>\n",
       "      <td>335</td>\n",
       "      <td>97</td>\n",
       "      <td>457</td>\n",
       "      <td>477</td>\n",
       "      <td>141</td>\n",
       "      <td>205</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4   5    6    7    8    9  ...  503  504  505  506  \\\n",
       "0  227  396  478  198  185  63  345  305   42  406  ...  132  434   21  317   \n",
       "1  158  366  460  202  231  38  356  277  109  397  ...  133  430   33  296   \n",
       "2  230  381  453  261  195   9  311  292   80  383  ...   97  437   15  320   \n",
       "3  207  342  480  246  198   0  321  261  112  381  ...   57  434   18  341   \n",
       "4  208  347  487  299  213   5  324  327   54  374  ...  167  437   89  355   \n",
       "5  142  382  467  173  167  64  337  236  103  388  ...  116  417   32  314   \n",
       "6  143  376  460  257  224  17  382  286   64  398  ...   95  436  102  342   \n",
       "7  176  344  482  255  230   3  313  267  159  380  ...  117  443   62  339   \n",
       "8  207  340  457  286  225  48  373  212   59  395  ...  131  406   55  362   \n",
       "9  119  368  478  257  176  87  382  311   90  355  ...  102  444   17  335   \n",
       "\n",
       "   507  508  509  510  511 run  \n",
       "0   78  452  474  140  176   0  \n",
       "1   92  412  464  136  100   1  \n",
       "2   87  424  467  212  170   2  \n",
       "3  123  435  478  237   86   3  \n",
       "4   28  411  456  191  113   4  \n",
       "5  106  428  472  219   71   5  \n",
       "6  110  453  466  163  150   6  \n",
       "7   65  421  468  157  168   7  \n",
       "8  122  456  435  241  127   8  \n",
       "9   97  457  477  141  205   9  \n",
       "\n",
       "[10 rows x 513 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average ranking of dimensions after 10 runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159    510.6\n",
       "409    510.4\n",
       "305    507.8\n",
       "275    507.7\n",
       "378    507.2\n",
       "387    506.5\n",
       "260    505.2\n",
       "465    504.3\n",
       "462    501.3\n",
       "374    500.2\n",
       "dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_weights.iloc[:, :512].mean().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77     19.9\n",
       "356    20.4\n",
       "190    23.0\n",
       "265    24.0\n",
       "136    25.7\n",
       "432    27.1\n",
       "59     27.1\n",
       "79     27.3\n",
       "163    28.0\n",
       "17     28.4\n",
       "dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_weights.iloc[:, :512].mean().sort_values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3de7a084b318d7b8bf96005cb5db4da14a27f60df0465391ef48a4c336f03bfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
